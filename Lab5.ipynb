{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Język Python - Laboratorium 5.\n",
    "## Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sklearn = Scikit learn - algorytmy uczenia maszynowego dla Pythona\n",
    "\n",
    "### Obszary zastosowań:\n",
    "- klasyfikacja\n",
    "- regresja\n",
    "- klasteryzacja\n",
    "- redukcja wymiarów\n",
    "- metaoptymalizacja - dobór parametrów modelu\n",
    "- preprocessing - normalizacja, dyskretyzacja etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Uzupełniające biblioteki:\n",
    "- numpy\n",
    "- pandas - formaty plików oraz eksploracja danych\n",
    "- matplotlib - wizualizacja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Alternatywy:\n",
    "- TensorFlow (Keras)\n",
    "- pyTorch (sieci neuronowe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ogólna procedura\n",
    "\n",
    "1. Przygotować wektory danych treningowych i testowych\n",
    "1. Zainstancjonować wybrany model\n",
    "1. Wywołać metodę `fit` od zbioru treningowego\n",
    "1. Opcjonalnie sprawdzić jakość modelu przy pomocy metody `score` (znaczenie zależne od modelu; im większa wartość tym lepiej)\n",
    "1. Stosować w systemie docelowym metodę `predict`/`transform`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ocena klasyfikacji binarnej:\n",
    "\n",
    "$$ Accuracy = \\frac{\\text{przypadki poprawnie zaklasyfikowane}}{\\text{wszystkie przypadki}} = \\frac{|TP| + |TN|}{|TP| + |TN| + |FP| + |FN|} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- klasyfikator dał odpowiedź 1, poprawna odpowiedź 1 - True Positive\n",
    "- klasyfikator dał odpowiedź 0, poprawna odpowiedź 0 - True Negative\n",
    "- klasyfikator dał odpowiedź 1, poprawna odpowiedź 0 - False Positive (błąd typu I)\n",
    "- klasyfikator dał odpowiedź 0, poprawna odpowiedź 1 - False Negative (błąd typu II)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$ Precision = \\frac{|TP|}{|TP| + |FP|} $$\n",
    "\n",
    "$$ Recall = \\frac{|TP|}{|TP|+|FN|} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "F1 - średnia harmoniczna precyzji i recall\n",
    "\n",
    "$$ \\frac{2}{f_1} = \\frac{1}{prec} + \\frac{1}{rec} \\Rightarrow f_1 = \\frac{2·prec·rec}{prec + rec} $$\n",
    "\n",
    "Uogólniając (na średnią ważoną):\n",
    "\n",
    "$$ F_ß = \\frac{(1+ß²)·prec·rec}{ß²·prec+rec} $$ (im większa ß tym **mniejsze** znaczenie ma precyzja)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ocena klasyfikacji wieloklasowej:\n",
    "\n",
    "Macro-average precision - obliczamy precyzję dla poszczególnych klas i liczymy średnią arytmetyczną"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ocena klasteryzacji\n",
    "(https://opendatascience.com/assessment-metrics-clustering-algorithms/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Davies-Bouldin Index\n",
    "![Davies-Bouldin Index](https://miro.medium.com/max/600/0*HeBUi2HtW8wnpnya.png)\n",
    "\n",
    "sigma - średnia odległość od centroidu klastra\n",
    "\n",
    "c - centroid\n",
    "\n",
    "mniejszy jest lepszy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Dunn Index\n",
    "![Dunn Index](https://miro.medium.com/max/600/0*z5AEKQ8GLqUw0G0D.png)\n",
    "\n",
    "d - odległość między klastrami\n",
    "\n",
    "d' - rozmiar klastra\n",
    "\n",
    "większy jest lepszy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Przykłady\n",
    "\n",
    "#### Klasyfikacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Gael Varoquaux <gael dot varoquaux at normalesup dot org>\n",
    "# modified for the purpose of this notebook\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets, svm, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "print(digits.images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To apply a classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "# Split data into train and test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, digits.target, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Create a classifier: a support vector classifier\n",
    "classifier = svm.SVC(gamma=0.001)\n",
    "# We learn the digits on the first half of the digits\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Now predict the value of the digit on the second half:\n",
    "predicted = classifier.predict(X_test)\n",
    "\n",
    "print(\"Classification report for classifier {}:\\n{}\\n\".format(\n",
    "    classifier, metrics.classification_report(y_test, predicted))\n",
    "     )\n",
    "# disp = metrics.plot_confusion_matrix(classifier, X_test, y_test)\n",
    "# disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "# print(\"Confusion matrix:\\n%s\" % disp.confusion_matrix)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regresja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n",
    "# modified for the purpose of this notebook\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "# #############################################################################\n",
    "# Generate sample data\n",
    "X = 80 * rng.rand(320000, 1)\n",
    "y = (np.sin(X) + np.sin(X/4)).ravel()\n",
    "\n",
    "# Add noise to targets\n",
    "# y[::5] += 3 * (0.5 - rng.rand(X.shape[0] // 5))\n",
    "\n",
    "X_plot = np.linspace(0, 120, 600000)[:, None]\n",
    "\n",
    "# #############################################################################\n",
    "# Fit regression model\n",
    "train_size = 100\n",
    "svr = GridSearchCV(SVR(kernel='rbf', gamma=0.1),\n",
    "                   param_grid={\"C\": [1e0, 1e1, 1e2, 1e3],\n",
    "                               \"gamma\": np.logspace(-2, 2, 5)},\n",
    "                   cv=3)\n",
    "\n",
    "kr = GridSearchCV(KernelRidge(kernel='rbf', gamma=0.1),\n",
    "                  param_grid={\"alpha\": [1e0, 0.1, 1e-2, 1e-3],\n",
    "                              \"gamma\": np.logspace(-2, 2, 5)},\n",
    "                  cv=3)\n",
    "\n",
    "svr.fit(X[:train_size], y[:train_size])\n",
    "\n",
    "kr.fit(X[:train_size], y[:train_size])\n",
    "\n",
    "y_svr = svr.predict(X_plot)\n",
    "\n",
    "y_kr = kr.predict(X_plot)\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Look at the results\n",
    "# sv_ind = svr.best_estimator_.support_\n",
    "# plt.scatter(X[sv_ind], y[sv_ind], c='r', s=50, label='SVR support vectors',\n",
    "#             zorder=2, edgecolors=(0, 0, 0))\n",
    "plt.scatter(X[:100], y[:100], c='k', label='data', zorder=1,\n",
    "            edgecolors=(0, 0, 0))\n",
    "plt.plot(X_plot, y_svr, c='r',\n",
    "         label='SVR')\n",
    "plt.plot(X_plot, y_kr, c='g',\n",
    "         label='KRR')\n",
    "plt.xlabel('data')\n",
    "plt.ylabel('target')\n",
    "plt.title('SVR versus Kernel Ridge')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Klasteryzacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_mean_shift.html#sphx-glr-auto-examples-cluster-plot-mean-shift-py\n",
    "import numpy as np\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth, AgglomerativeClustering\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# #############################################################################\n",
    "# Generate sample data\n",
    "centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "X, _ = make_blobs(n_samples=4000, centers=centers, cluster_std=0.6)\n",
    "\n",
    "# #############################################################################\n",
    "# Compute clustering with MeanShift\n",
    "\n",
    "# The following bandwidth can be automatically detected using\n",
    "bandwidth = estimate_bandwidth(X, quantile=0.2, n_samples=500)\n",
    "model = MeanShift(bandwidth=bandwidth)\n",
    "\n",
    "# model = AgglomerativeClustering()\n",
    "\n",
    "model.fit(X)\n",
    "\n",
    "try:\n",
    "    cluster_centers = model.cluster_centers_\n",
    "except AttributeError:\n",
    "    pass  # ignore\n",
    "\n",
    "labels = model.labels_\n",
    "labels_unique = np.unique(labels)\n",
    "n_clusters_ = len(labels_unique)\n",
    "\n",
    "print(\"number of estimated clusters : {}\".format(n_clusters_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot result\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "colors = cycle('bgrcmyk')\n",
    "for k, col in zip(range(n_clusters_), colors):\n",
    "    my_members = labels == k\n",
    "    plt.plot(X[my_members, 0], X[my_members, 1], col + '.')\n",
    "    try:\n",
    "        cluster_center = cluster_centers[k]\n",
    "        print(cluster_center)\n",
    "        plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n",
    "                 markeredgecolor='k', markersize=14)\n",
    "    except NameError:\n",
    "        pass  # ignore\n",
    "plt.title('Estimated number of clusters: {}'.format(n_clusters_))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redukcja wymiarów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors: Gaël Varoquaux, Kyle Kastner\n",
    "# compiled and modified\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import datasets\n",
    "from sklearn import decomposition\n",
    "\n",
    "n_components = 2\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "pca = decomposition.PCA(n_components=n_components)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "if n_components == 2:\n",
    "\n",
    "    colors = ['navy', 'turquoise', 'darkorange']\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    for color, i, target_name in zip(colors, [0, 1, 2], iris.target_names):\n",
    "        plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1],\n",
    "                    color=color, lw=2, label=target_name)\n",
    "\n",
    "    plt.title(\"PCA of iris dataset\")\n",
    "    plt.legend(loc=\"best\", shadow=False, scatterpoints=1)\n",
    "    plt.axis([-4, 4, -1.5, 1.5])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    fig = plt.figure(1, figsize=(4, 3))\n",
    "    plt.clf()\n",
    "    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
    "    plt.cla()\n",
    "\n",
    "    for name, label in [('Setosa', 0), ('Versicolour', 1), ('Virginica', 2)]:\n",
    "        ax.text3D(X_pca[y == label, 0].mean(),\n",
    "                  X_pca[y == label, 1].mean() + 1.5,\n",
    "                  X_pca[y == label, 2].mean(), name,\n",
    "                  horizontalalignment='center',\n",
    "                  bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))\n",
    "    # Reorder the labels to have colors matching the cluster results\n",
    "    y = np.choose(y, [1, 2, 0]).astype(np.float)\n",
    "    ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=y, cmap=plt.cm.nipy_spectral,\n",
    "               edgecolor='k')\n",
    "\n",
    "    ax.w_xaxis.set_ticklabels([])\n",
    "    ax.w_yaxis.set_ticklabels([])\n",
    "    ax.w_zaxis.set_ticklabels([])\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Zapis i odczyt modelu\n",
    "Sposób 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "dump(classifier, 'filename.joblib')\n",
    "# ...\n",
    "classifier2 = load('filename.joblib')\n",
    "predicted = classifier2.predict(X_test)\n",
    "\n",
    "print(\"Classification report for classifier {}:\\n{}\\n\".format(\n",
    "    classifier2, metrics.classification_report(y_test, predicted))\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Sposób 2:\n",
    "\n",
    "https://scikit-learn.org/stable/related_projects.html#related-projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Zadanie:\n",
    "1. Napisać program, który rozpoznaje język wprowadzonego fragmentu tekstu, używając przynajmniej 3 różnych algorytmów\n",
    "2. Ocenić poprawność wyników (precision, recall, f1, accuracy)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
